{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "P814G_OLZ0Hf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">YouTube Transcript Summarizer Using BART + NLTK</span>"
      ],
      "metadata": {
        "id": "SedOh6M2OAxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook extracts a YouTube video's transcript, cleans it, splits it into sentence-aware chunks using NLTK, and summarizes it using the BART model (`facebook/bart-large-cnn`)."
      ],
      "metadata": {
        "id": "TTdMgsRqOIQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Install Dependencies</span>"
      ],
      "metadata": {
        "id": "K57b36uEu4jQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Bn6myztGEn",
        "outputId": "e636e83e-143e-4544-cd49-42edac9bcdf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show youtube-transcript-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U939PlIOpWan",
        "outputId": "a7e3dbad-bcdd-4573-b568-d803cc390696"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: youtube-transcript-api\n",
            "Version: 1.2.3\n",
            "Summary: This is an python API which allows you to get the transcripts/subtitles for a given YouTube video. It also works for automatically generated subtitles, supports translating subtitles and it does not require a headless browser, like other selenium based solutions do!\n",
            "Home-page: https://github.com/jdepoix/youtube-transcript-api\n",
            "Author: Jonas Depoix\n",
            "Author-email: jonas.depoix@web.de\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: defusedxml, requests\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.52.4 torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pirvyFpCvGZJ",
        "outputId": "17dbb3a7-15b0-4067-c09f-cc9ae620e064"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.52.4 in /usr/local/lib/python3.12/dist-packages (4.52.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.4) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.4) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.4) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.4) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.4) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDEl0l0RONDR",
        "outputId": "370a81a2-94f3-4ec8-e76e-c5d4bf8399c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shZf7rJM6DBa",
        "outputId": "a458c7a8-62c1-49ea-ccea-1e23c232bec0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `youtube-transcript-api` ‚Üí fetches YouTube captions  \n",
        "- `transformers` ‚Üí loads BART model  \n",
        "- `sentencepiece` ‚Üí required for BART tokenizer  \n",
        "- `nltk` ‚Üí for sentence tokenization (better chunking)  "
      ],
      "metadata": {
        "id": "wDIgdhCDOqTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Imports</span>"
      ],
      "metadata": {
        "id": "i7Pi6G0LvJ14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse, parse_qs\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "import re"
      ],
      "metadata": {
        "id": "SyNbutWiO2th"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- URL parsing for video ID  \n",
        "- YouTube transcript extraction  \n",
        "- BART tokenizer + model  \n",
        "- NLTK for sentence tokenization  \n",
        "- Regex for cleaning"
      ],
      "metadata": {
        "id": "q-MpFGBOPGa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Download NLTK Model</span>"
      ],
      "metadata": {
        "id": "BobHOvpfPdOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayNopE7BPkYy",
        "outputId": "6d0f2b1a-4106-4975-9c59-d8138c880931"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the `punkt` tokenizer which allows us to split text into sentences for better summarization."
      ],
      "metadata": {
        "id": "3E_GxzSRPl8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Extracting the Video ID</span>"
      ],
      "metadata": {
        "id": "YTiWLKDUvRcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_video_id(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract YouTube video ID from any YouTube URL format.\n",
        "    \"\"\"\n",
        "    parsed = urlparse(url)\n",
        "\n",
        "    # Standard YouTube link\n",
        "    if parsed.hostname in [\"www.youtube.com\", \"youtube.com\"]:\n",
        "        return parse_qs(parsed.query).get(\"v\", [None])[0]\n",
        "\n",
        "    # Shortened youtu.be link\n",
        "    if parsed.hostname == \"youtu.be\":\n",
        "        return parsed.path.lstrip(\"/\")\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "Ngnwy-UyvZxm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç Extract Video ID\n",
        "Accepts both:\n",
        "- `https://youtube.com/watch?v=ID`\n",
        "- `https://youtu.be/ID`\n",
        "\n",
        "Returns only the video ID."
      ],
      "metadata": {
        "id": "61B1jOp7P8gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Fetch the Youtube Transcript</span>"
      ],
      "metadata": {
        "id": "vzBzSKW6vcyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_transcript(video_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetch transcript as a single text string using the new API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ytt_api = YouTubeTranscriptApi()  # create instance\n",
        "        fetched = ytt_api.fetch(video_id)  # fetch transcript\n",
        "        raw = fetched.to_raw_data()  # convert to list of dicts\n",
        "        text = \" \".join([entry[\"text\"] for entry in raw])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching transcript: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "tiTPxZOivgAO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìú Fetch Transcript\n",
        "Using `YouTubeTranscriptApi.get_transcript()`, we return the entire transcript merged into a single string.\n"
      ],
      "metadata": {
        "id": "9DK4BoCjQ4Br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Clean Transcript Text</span>"
      ],
      "metadata": {
        "id": "oVRevL3OQ9vU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean transcript: remove extra spaces, newlines, and bracketed tags.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # remove things like [Music]\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "KltlBtWVRGdN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßπ Cleaning Transcript\n",
        "Removes:\n",
        "- Extra spaces  \n",
        "- Tags like `[Music]`, `[Applause]`  \n",
        "- Newline artifacts  \n",
        "\n",
        "Creates a clean, smooth transcript for summarization.\n"
      ],
      "metadata": {
        "id": "q4DnBAPXVIwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Sentence-Aware Chunking (NLTK)</span>\n"
      ],
      "metadata": {
        "id": "DHPdVqyrVO8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, max_words=900):\n",
        "    \"\"\"\n",
        "    Split transcript into chunks using NLTK sentence tokenization.\n",
        "    Prevents mid-sentence cuts and improves summary coherence.\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        count = len(words)\n",
        "\n",
        "        if current_count + count > max_words:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_count = 0\n",
        "\n",
        "        current_chunk.append(sentence)\n",
        "        current_count += count\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "dOXjS2iIVV5V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÇÔ∏è Sentence-Aware Chunking (NLTK)\n",
        "This version uses **NLTK sentence tokenization** so chunks never cut off mid-sentence.\n",
        "\n",
        "Benefits:\n",
        "- Better coherence  \n",
        "- Higher BART summary quality  \n",
        "- Less model confusion  "
      ],
      "metadata": {
        "id": "D7lgl65eVaZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Load BART Model</span>"
      ],
      "metadata": {
        "id": "8VKihTSk0Nwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Model link](https://huggingface.co/facebook/bart-large-cnn)"
      ],
      "metadata": {
        "id": "zLQ4-FSu0WEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7BsqY5oWcVN",
        "outputId": "0c5fc6bb-89b5-4590-ca4f-dc483f680338"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ Load BART Model\n",
        "Loads the `facebook/bart-large-cnn` model trained specifically for summarization tasks."
      ],
      "metadata": {
        "id": "KLAXjxA2Ws9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Summarize One Chunk</span>"
      ],
      "metadata": {
        "id": "cbmcOKSkWtuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline, AutoTokenizer"
      ],
      "metadata": {
        "id": "zmkYFZ-gzl7_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_chunk(text: str) -> str:\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=200,\n",
        "        min_length=60,\n",
        "        num_beams=4,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "0bF-OExrzbHX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù Summarizing One Chunk\n",
        "Uses BART with beam search for a high-quality summary:\n",
        "- Beam search (4 beams)  \n",
        "- Minimum length enforced  \n",
        "- Early stopping enabled  "
      ],
      "metadata": {
        "id": "P814G_OLZ0Hf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function:\n",
        "\n",
        "1. Tokenizes the chunk of text\n",
        "\n",
        "2. Truncates it to fit the model's maximum sequence length\n",
        "\n",
        "3. Generates a summary using BART\n",
        "\n",
        "4. Decodes the summary tokens back into readable text"
      ],
      "metadata": {
        "id": "djk-JHeEa7cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Tokenizer Parameters (`tokenizer.encode`)**\n",
        "\n",
        "`text`\n",
        "\n",
        "- The text chunk to summarize.\n",
        "\n",
        "`return_tensors=\"pt\"`\n",
        "\n",
        "- Returns the tokenized output as PyTorch tensors.\n",
        "\n",
        "- Required because the model expects tensor input.\n",
        "\n",
        "`max_length=1024`\n",
        "\n",
        "- BART-large has a maximum input length of 1024 tokens.\n",
        "\n",
        "- This prevents overflow errors.\n",
        "\n",
        "- If the chunk is longer, it will be truncated.\n",
        "\n",
        "`truncation=True`\n",
        "\n",
        "- Ensures that any text exceeding 1024 tokens is safely cut off.\n",
        "\n",
        "- Prevents crashes and overflows.\n",
        "\n",
        "2. **Model Generation Parameters (`model.generate`)**\n",
        "\n",
        "`max_length=200`\n",
        "\n",
        "- The maximum number of tokens allowed in the generated summary.\n",
        "\n",
        "- Higher value ‚Üí longer summary\n",
        "\n",
        "- Lower value ‚Üí shorter summary\n",
        "\n",
        "`min_length=60`\n",
        "\n",
        "- The minimum number of tokens the model must generate.\n",
        "\n",
        "- Prevents overly short or meaningless summaries.\n",
        "\n",
        "`num_beams=4`\n",
        "\n",
        "- This activates beam search, a smarter search algorithm for generation.\n",
        "\n",
        "- The model considers 4 possible next-token paths at every step.\n",
        "\n",
        "- Higher values improve quality but increase compute time.\n",
        "\n",
        "- Common values: 3-6\n",
        "\n",
        "- 4 is a good balance between quality and speed.\n",
        "\n",
        "`length_penalty=2.0`\n",
        "\n",
        "- Controls how much the model is penalized for generating longer sequences.\n",
        "\n",
        "- Values > 1.0 encourage the model to be more concise.\n",
        "\n",
        "- Values < 1.0 allow longer outputs.\n",
        "\n",
        "Why 2.0?\n",
        "It produces shorter, more focused summaries.\n",
        "\n",
        "`early_stopping=True`\n",
        "\n",
        "- Stops beam search when all beams finish generating.\n",
        "\n",
        "- Makes generation faster and more predictable.\n",
        "\n",
        "3. **Decoding Parameters (`tokenizer.decode`)**\n",
        "\n",
        "`summary_ids[0]`\n",
        "\n",
        "- The generated sequence of token IDs.\n",
        "\n",
        "`skip_special_tokens=True`\n",
        "\n",
        "- Removes tokens like `<s>`, `</s>`, `<pad>`, `<unk>`.\n",
        "\n",
        "- Ensures the output is clean, readable text."
      ],
      "metadata": {
        "id": "Bm7JFLwbbHJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Full Pipeline Function</span>\n"
      ],
      "metadata": {
        "id": "5dy9L8lW4XDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_youtube_video(url: str):\n",
        "    video_id = extract_video_id(url)\n",
        "\n",
        "    if not video_id:\n",
        "        return \"‚ùå Invalid YouTube URL.\"\n",
        "\n",
        "    text = fetch_transcript(video_id)\n",
        "    if not text:\n",
        "        return \"‚ùå Transcript unavailable.\"\n",
        "\n",
        "    cleaned = clean_text(text)\n",
        "    chunks = chunk_text(cleaned)\n",
        "\n",
        "    print(f\"Total words: {len(cleaned.split())}\")\n",
        "    print(f\"Total chunks: {len(chunks)}\\n\")\n",
        "\n",
        "    summaries = []\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"Summarizing chunk {i}/{len(chunks)}...\")\n",
        "        summaries.append(summarize_chunk(chunk))\n",
        "\n",
        "    final_summary = \"\\n\\n\".join(summaries)\n",
        "    return final_summary"
      ],
      "metadata": {
        "id": "uEI9DeGodOh2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üöÄ Full Summarization Pipeline\n",
        "1. Extract video ID  \n",
        "2. Download transcript  \n",
        "3. Clean the text  \n",
        "4. Sentence-based chunking (NLTK)  \n",
        "5. Summarize each chunk individually  \n",
        "6. Combine all summaries into a final result  "
      ],
      "metadata": {
        "id": "ZjRVg01zdyx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:orange\">Execute Summarizer</span>\n"
      ],
      "metadata": {
        "id": "j6SQ1Pxo0H0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link of the video:https://www.youtube.com/watch?v=IzQ2siryQrM"
      ],
      "metadata": {
        "id": "1uzvd_Qks19t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_url = input(\"Enter YouTube video URL: \")\n",
        "summary = summarize_youtube_video(video_url)\n",
        "\n",
        "print(\"\\n===== FINAL SUMMARY =====\\n\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqkTgVT4sLHo",
        "outputId": "9cf66bf2-e992-4721-c008-98f571a26e09"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter YouTube video URL: https://www.youtube.com/watch?v=IzQ2siryQrM\n",
            "Total words: 261\n",
            "Total chunks: 1\n",
            "\n",
            "Summarizing chunk 1/1...\n",
            "\n",
            "===== FINAL SUMMARY =====\n",
            "\n",
            "Adults typically need seven to nine hours of sleep for maximum brain performance. Too little sleep negatively affects your ability to remember and concentrate. It can also make you moodier and more irritable and increase the risk of anxiety and depression. To ensure you're getting enough sleep, practice good sleep hygiene.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ñ∂Ô∏è Run the Summarizer\n",
        "Paste any YouTube link to generate a coherent summary powered by BART and NLTK.\n"
      ],
      "metadata": {
        "id": "aGd17br4eJx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "--NLHGR1YMKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Not Use pipeline like in the intern and in the older video?"
      ],
      "metadata": {
        "id": "Gte3rfygYNKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use:\n",
        "```\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "```\n",
        "\n",
        "but it comes with drawbacks:"
      ],
      "metadata": {
        "id": "KVBEK1D6YXNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëç Pros\n",
        "\n",
        "- Very simple\n",
        "\n",
        "- One-line model loader\n",
        "\n",
        "- Clean syntax\n",
        "\n",
        "üëé Cons\n",
        "\n",
        "- Slower for repeated summarization\n",
        "\n",
        "- Less control over model behavior\n",
        "\n",
        "- More internal ‚Äúhidden rules‚Äù\n",
        "\n",
        "- Easier to get inconsistent summaries with long text chunks\n",
        "\n",
        "For a multi-chunk, long-document summarizer, the direct model method is the better engineering choice."
      ],
      "metadata": {
        "id": "ehgovoHwYUp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary Table"
      ],
      "metadata": {
        "id": "gdaBZRk6YvlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature                   | `BartForConditionalGeneration` | `pipeline(\"summarization\")` |\n",
        "| ------------------------- | ------------------------------ | --------------------------- |\n",
        "| Speed (multiple calls)    | ‚≠ê Faster                       | ‚ö†Ô∏è Slower                   |\n",
        "| Control over parameters   | ‚≠ê Full control                 | ‚ö†Ô∏è Limited                  |\n",
        "| Good for long transcripts | ‚≠ê Yes                          | ‚ö†Ô∏è Not ideal                |\n",
        "| Hidden defaults           | None                           | Many                        |\n",
        "| Best for this project     | ‚úÖ Yes                          | Optional                    |\n"
      ],
      "metadata": {
        "id": "KHa3LukTYzIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline(\"summarization\") approach hides many generation settings behind defaults.\n",
        "For example:\n",
        "\n",
        "- `max_length`\n",
        "\n",
        "- `min_length`\n",
        "\n",
        "- `num_beams`\n",
        "\n",
        "- `length_penalty`\n",
        "\n",
        "- `early_stopping`\n",
        "\n",
        "When summarizing long transcripts in multiple chunks, having full control over these settings gives better quality summaries.\n",
        "\n",
        "Using:\n",
        "```\n",
        "model.generate(...)\n",
        "```\n",
        "\n",
        "ensures the model follows the exact parameters we specify."
      ],
      "metadata": {
        "id": "FCgFTZTPY1U-"
      }
    }
  ]
}